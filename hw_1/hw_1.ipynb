{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62289957-0b83-45f1-9227-7c04d3b1694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_regression(n_samples=30_000, n_features=100, noise=5, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cc092a-a5f4-46a2-a031-ec239fab823a",
   "metadata": {},
   "source": [
    "Задача 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a2676a1-331c-4b46-8934-e94e86937be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression_castom():\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate=0.1,\n",
    "        max_itter=10,\n",
    "        log=True,\n",
    "    ):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_itter = max_itter\n",
    "        self.log = log\n",
    "    \n",
    "    @staticmethod\n",
    "    def __init_weights(n_in, n_out):\n",
    "        \"\"\"\n",
    "        Normal Xavier initialization\n",
    "        \"\"\"\n",
    "        return np.random.normal(loc=0.0, scale=np.sqrt(2/(n_in +n_out)), size=(n_out, n_in))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_loss(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Get MSE loss\n",
    "        \"\"\"\n",
    "        assert y_true.size == y_pred.size\n",
    "        return np.sum((y_true - y_pred)**2) / y_true.size\n",
    "\n",
    "    @staticmethod\n",
    "    def get_loss_grad(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Get MSE loss\n",
    "        \"\"\"\n",
    "        assert y_true.size == y_pred.size\n",
    "        return -2*(y_true - y_pred)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        size, n_fetures = X.shape\n",
    "        self.b = 0 \n",
    "        self.weights = self.__init_weights(n_in = n_fetures, n_out = 1)\n",
    "        for _ in range(self.max_itter):\n",
    "            predictions = (X @ self.weights.T + self.b).flatten()\n",
    "            grads = np.clip(self.get_loss_grad(y, predictions),-1,1)\n",
    "            self.weights = self.weights - self.learning_rate * (grads.T @ X)\n",
    "            self.b = self.b - self.learning_rate * np.mean(grads.T)\n",
    "            if self.log:\n",
    "                print(self.get_loss(y, predictions))\n",
    "    def predict(self, X):\n",
    "        return (X @ self.weights.T + self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e4c0068-2fe7-46b0-b5ad-d92716a51828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.43345560250734\n"
     ]
    }
   ],
   "source": [
    "clf = LinearRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "print(mean_squared_error(clf.predict(X_val), y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6de04ce9-f128-4383-be28-52b3563d2346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.50977941086595"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression_castom(0.0001, 10000, False)\n",
    "model.train(X_train, y_train)\n",
    "mean_squared_error(model.predict(X_val), y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a62d244-9afa-46d0-bba0-29e02e22a3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPLinearRegression():\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_layer_sizes=(100, 200, 300),\n",
    "        learning_rate=0.1,\n",
    "        max_itter=10\n",
    "    ):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_itter = max_itter\n",
    "\n",
    "    @staticmethod\n",
    "    def __init_weights(n_in, n_out):\n",
    "        \"\"\"\n",
    "        Normal Xavier initialization\n",
    "        \"\"\"\n",
    "        return np.random.normal(loc=0.0, scale=np.sqrt(2/(n_in+n_out)), size=(n_in, n_out))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_loss(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Get MSE loss\n",
    "        \"\"\"\n",
    "        assert y_true.size == y_pred.size\n",
    "        cost = np.mean((y_pred - y_true)**2)\n",
    "        return cost\n",
    "\n",
    "    @staticmethod\n",
    "    def get_loss_grad(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Get MSE loss\n",
    "        \"\"\"\n",
    "        assert y_true.size == y_pred.size\n",
    "        return 2*(y_pred - y_true)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_relu(x):\n",
    "        return np.maximum(0, x)\n",
    "    @staticmethod\n",
    "    def get_relu_diff(x):\n",
    "        return np.where(x <= 0, 0, 1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X\n",
    "        self.later_prom_res = []\n",
    "        for layer in self.weights:\n",
    "            index, bais, layer_weights, func = layer\n",
    "            pre_activ =  X @ layer_weights + bais\n",
    "            if func == \"RELU\":\n",
    "                X = self.get_relu(pre_activ.copy())\n",
    "            elif func is None:\n",
    "                X = pre_activ\n",
    "            else:\n",
    "                raise ValueError()\n",
    "            # print(\"X:\", X.shape)\n",
    "            self.later_prom_res.append([pre_activ, X.copy()])\n",
    "        return X\n",
    "\n",
    "\n",
    "    def train(self, X, y):\n",
    "        # self.iters = np.linspace(0.001,0.00001,self.max_itter)\n",
    "        X_, y_ = X, y\n",
    "\n",
    "        y_ = y_.reshape(-1,1)\n",
    "        size, n_fetures = X.shape\n",
    "        self.weights = []\n",
    "\n",
    "        prev_hid_layer_size = n_fetures\n",
    "        for index, curr_hidden_layer_size in enumerate(self.hidden_layer_sizes):\n",
    "            weight = self.__init_weights(n_in=prev_hid_layer_size, n_out=curr_hidden_layer_size)\n",
    "            bais = self.__init_weights(n_in=1, n_out=curr_hidden_layer_size)\n",
    "            self.weights.append([index, bais, weight, \"RELU\"])\n",
    "            prev_hid_layer_size = curr_hidden_layer_size\n",
    "        last_layer = self.__init_weights(n_in=prev_hid_layer_size, n_out=1)\n",
    "        bais = self.__init_weights(n_in=1, n_out=1)\n",
    "        self.weights.append([index + 1, bais, last_layer, None])  \n",
    "        for i in range(0, self.max_itter):\n",
    "            result_forward = self.forward(X_)\n",
    "            error = self.get_loss(y_, result_forward.reshape(-1,1))\n",
    "            if i % 200 == 0:\n",
    "                print(i, error)\n",
    "            self.backward(X_,y_)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X).reshape(-1,1)\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        y_predicted =  self.later_prom_res[-1][1].reshape(-1,1)\n",
    "        lst_deltas = [self.get_loss_grad(y, y_predicted)]\n",
    "        \n",
    "        for layer_index in range(len(self.weights) - 2, -1, -1):\n",
    "            w_l_plis_1 = self.weights[layer_index + 1][2]\n",
    "            delta  = lst_deltas[-1] @ w_l_plis_1.T\n",
    "            delta = delta * self.get_relu_diff(self.later_prom_res[layer_index][0])\n",
    "            lst_deltas.append(delta)\n",
    "        \n",
    "        lst_deltas = list(reversed(lst_deltas))\n",
    "        \n",
    "        for layer_index in range(len(lst_deltas) - 1, -1, -1):\n",
    "            prev_a_x = self.later_prom_res[layer_index - 1][1] if layer_index > 0 else X\n",
    "            delta_l_plus_1 = lst_deltas[layer_index]\n",
    "            mat_mull =  np.clip((prev_a_x.T @ delta_l_plus_1), -10, 10)\n",
    "            self.weights[layer_index][2] = self.weights[layer_index][2] - self.learning_rate * mat_mull\n",
    "            self.weights[layer_index][1] = self.weights[layer_index][1] - self.learning_rate * np.mean(np.clip(delta_l_plus_1,-10,10), axis=0, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24471b16-63b2-4c2c-ae0e-32a9273945bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 34675.97471965502\n",
      "200 19389.96535419596\n",
      "400 952.4779159160378\n",
      "600 388.49941316246463\n",
      "800 303.15514350074284\n",
      "1000 240.01893035497937\n",
      "1200 210.84419252500572\n",
      "1400 186.61656063769848\n",
      "1600 165.13508166907206\n",
      "1800 146.7059773918965\n",
      "2000 130.43096566832827\n",
      "2200 117.36227249541639\n",
      "2400 105.46618402690866\n",
      "2600 96.34341646020519\n",
      "2800 88.81202606442464\n",
      "3000 82.89066557904634\n",
      "3200 78.32117732717037\n",
      "3400 74.41940569260372\n",
      "3600 71.68585745828963\n",
      "3800 68.51164742116904\n",
      "4000 66.44502339392078\n",
      "4200 64.64856517614345\n",
      "4400 63.198467590491305\n",
      "4600 62.486781023875096\n",
      "4800 61.97113955415869\n",
      "5000 61.27812346020093\n",
      "5200 60.86962075522099\n",
      "5400 60.62897940699146\n",
      "5600 60.349601787962456\n",
      "5800 60.26279342357896\n",
      "6000 60.158051225685206\n",
      "6200 60.08983529147338\n",
      "6400 59.93687838489051\n",
      "6600 59.81669131393653\n",
      "6800 59.73957405184829\n",
      "7000 59.63379362960576\n",
      "7200 59.5653721165485\n",
      "7400 59.41294413596589\n",
      "7600 59.34403224788187\n",
      "7800 59.169361776882894\n",
      "8000 59.045203703786136\n",
      "8200 58.98756080471048\n",
      "8400 58.90284214758398\n",
      "8600 58.65577735186798\n",
      "8800 58.35309733540106\n",
      "9000 58.316812302260814\n",
      "9200 58.208409293605946\n",
      "9400 57.997779050622356\n",
      "9600 57.79448398116905\n",
      "9800 57.78923856861272\n"
     ]
    }
   ],
   "source": [
    "mlpLinReg = MLPLinearRegression(\n",
    "    hidden_layer_sizes=(5,10,10),\n",
    "    learning_rate=0.0003,\n",
    "    max_itter=10000) \n",
    "mlpLinReg.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da655fe9-8ade-4a7e-a379-70f6b0848e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.991064033952334"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(mlpLinReg.predict(X_val), y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcc902e-0b57-4754-a0f5-3b2dff98c3f0",
   "metadata": {},
   "source": [
    "# Useful resourses:\n",
    "* http://neuralnetworksanddeeplearning.com/chap2.html\n",
    "* https://hackmd.io/@machine-learning/blog-post-cnnumpy-slow (use VPN)\n",
    "* https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b6becc-62de-4053-aba2-4a5ce1286d89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
